# ðŸ¤Ÿ AI Sign Language Translator (ISL & ASL)

![Visionary AI Banner](https://img.shields.io/badge/Status-Active-brightgreen)
![Version](https://img.shields.io/badge/Version-2.0-blue)
![Python](https://img.shields.io/badge/Python-3.8+-yellow)
![Framework](https://img.shields.io/badge/Framework-Streamlit-FF4B4B)
![AI-Engine](https://img.shields.io/badge/Neural_Engine-MediaPipe_|_TensorFlow-blueviolet)
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-Holistic-brightgreen.svg)](https://mediapipe.dev/)
[![Streamlit](https://img.shields.io/badge/Interface-Streamlit-red.svg)](https://streamlit.io/)
[![Docker](https://img.shields.io/badge/Deployment-Docker-blue.svg)](https://www.docker.com/)

> **Bridging the Communication Gap:** A state-of-the-art, real-time bidirectional translator that converts Sign Language gestures into text and audible speech using Computer Vision and Deep Learning.

---

## ðŸŒŸ Key Features
-   **Real-Time Gesture Recognition:** Leveraging MediaPipe Holistic for high-fidelity landmark detection.
-   **Bi-Directional Translation:**
    -   **Sign to Speech:** Dynamic gesture recognition âž¡ï¸ Text âž¡ï¸ Audio Synthesis.
    -   **Speech to Sign:** Voice recognition âž¡ï¸ Grammatical Parsing âž¡ï¸ Sign Visualization.
-   **Temporal Awareness:** Sequential LSTM-based architecture to capture the nuances of dynamic signs.
-   **Cross-Platform UI:** Modern, responsive dashboard built with Streamlit for low-latency feedback.
-   **Edge Optimized:** Designed for high FPS on standard webcams without requiring high-end GPUs.

---

## ðŸ—ï¸ Technical Architecture

### 1. The Intelligence Pipeline
The system processes data in a synchronized multi-stage pipeline:
1.  **Vision Engine:** Captures raw video frames at ~30 FPS via OpenCV.
2.  **Feature Extraction:** MediaPipe Holistic extracts 1,662 keypoints covering Hands, Pose, and Face mesh.
3.  **Normalization:** Keypoints are transposed to a relative coordinate system to handle variations in user distance and position.
4.  **Temporal Modeling:** A sliding window of 30 frames is fed into an **LSTM (Long Short-Term Memory)** network to predict gestures.
5.  **Confidence Thresholding:** Predictions are only voiced if the softmax probability exceeds a configurable threshold (e.g., 85%).

### 2. System Diagram
```mermaid
graph LR
    A[Webcam Feed] --> B[MediaPipe Holistic]
    B --> C{Landmark Extraction}
    C -->|Hand/Pose/Face| D[Sequence Buffer]
    D --> E[LSTM Deep Learning Model]
    E --> F[Softmax Prediction]
    F --> G[TTS Engine]
    G --> H[Audible Output]
```

---

## ðŸ“‚ Repository Structure
```bash
AI-Sign-Language-Translator/
â”œâ”€â”€ app/                  # Frontend Application Layer
â”‚   â””â”€â”€ app.py            # Streamlit Dashboard Entry
â”œâ”€â”€ data/                 # Dataset Management
â”‚   â”œâ”€â”€ raw/              # Raw .npy landmark sequences
â”‚   â””â”€â”€ processed/        # Validated training data
â”œâ”€â”€ models/               # Pre-trained Weights & Quantized Models
â”œâ”€â”€ notebooks/            # Research, EDA, and Model Training
â”œâ”€â”€ src/                  # Core Logic
â”‚   â”œâ”€â”€ collection/       # Custom Data Collection Engine
â”‚   â”œâ”€â”€ inference/        # Real-time Prediction Logic
â”‚   â””â”€â”€ utils/            # Signal Processing & Landmark Wrappers
â”œâ”€â”€ requirements.txt      # Production-grade Dependencies
â””â”€â”€ Dockerfile            # Containerization Config
```

---

## ðŸ“Š Performance Benchmarks
| Metric | Resolution | Latency | Accuracy (Val) |
| :--- | :--- | :--- | :--- |
| Hand Detection | 640x480 | ~12ms | 99.2% |
| Full Recognition | 640x480 | ~35ms | 94.5% |
| End-to-End Jitter | - | <50ms | - |

---
### ðŸ§° ð“ðžðœð¡ ð’ð­ðšðœð¤
**ðð«ð¨ð ð«ðšð¦ð¦ð¢ð§ð  & ðƒðšð­ðš ðð«ð¨ðœðžð¬ð¬ð¢ð§ð **
- Python 3.9+
- NumPy
- Pandas (Data Management)

**ðŒðšðœð¡ð¢ð§ðž ð‹ðžðšð«ð§ð¢ð§ð  & ðƒðžðžð© ð‹ðžðšð«ð§ð¢ð§ð **
- TensorFlow / Keras (LSTM & Bidirectional Architectures)
- MediaPipe Holistic (Pose, Hand, & Face Landmark Extraction)

**ðˆð§ð­ðžð«ðŸðšðœðž & ð•ð¢ð¬ð®ðšð¥ð¢ð³ðšð­ð¢ð¨ð§**
- Streamlit (Advanced Cyberpunk UI Implementation)
- OpenCV (Real-time Computer Vision)
- Altair & Matplotlib (Live Neural Metrics)

**ð’ð©ðžðžðœð¡ & ð€ð®ðð¢ð¨**
- gTTS (Google Text-to-Speech)
- pyttsx3 (Offline Neural Voice)
- SpeechRecognition (Audio Processing)

**ðƒðšð­ðšð¬ðžð­**
- **Custom ISL Corpus**: 170+ Classes, 12,000+ Samples.
- **Data Shape**: (Sequences: 30 Frames, Keypoints: 258 per frame).

### ð‚ð‡ð€ð‹ð‹ð„ðð†ð„ð’ ð…ð€ð‚ð„ðƒ
- **ðŒðšðœð¡ð¢ð§ðž ð‹ðžðšð«ð§ð¢ð§ð  ð‚ð¡ðšð¥ð¥ðžð§ð ðž**:
  Teaching the LSTM model to distinguish between spatially similar signs (e.g., "Hello" vs "Idle", "Thank You" vs "Please") was difficult. We implemented a **OneEuroFilter** for jitter reduction and used **Data Augmentation** to balance the dataset.

- **ð‘ðžðšð¥-ð“ð¢ð¦ðž ð‹ðšð­ðžð§ðœð² ð‚ð¡ðšð¥ð¥ðžð§ð ðž**:
  Running complex Deep Learning inference on every video frame caused lag. We solved this by implementing a **Multi-threaded Inference Engine** and a **Sliding Window Trigger** (every 3 frames) to effectively double the perceived FPS while maintaining high accuracy.

- **ð‚ð«ð¨ð¬ð¬-ðð¥ðšð­ðŸð¨ð«ð¦ ð€ð®ðð¢ð¨**:
  Managing Audio Drivers across Windows (WSL) and Native environments was tricky. We built a robust **Hybrid Audio Engine** that gracefully falls back between online (gTTS) and offline (pyttsx3) synthesis systems.

- **ð–ðžð›ðœðšð¦ & ð•ð¢ð¬ð¢ð¨ð§ ð’ð­ðšð›ð¢ð¥ð¢ð­ð² (ð‘ðžðšð¥-ð–ð¨ð«ð¥ð ðð¨ð¢ð¬ðž)**:
  - **Depth Loss**: A single 2D webcam cannot easily distinguish between "moving hand forward" and "hand getting bigger". We solved this using **Contextual Landmarks Normalization**.
  - **Motion Blur & Lighting**: Fast signs often blur in standard 30fps webcams, causing MediaPipe landmarks to vanish. Our **Hybrid Interpolation System** fills in these missing gaps to prevent the AI from seeing "zeros".
  - **Occlusion**: When hands cross over the face or each other, landmarks get confused. We implemented **Temporal Smoothing** to retain the last known valid position until reliable tracking returns.

---

## ðŸ“§ Contact
**Project Lead:** Shubham Kumawat
[GitHub Profile](https://github.com/shubhamkumawat789) | [LinkedIn](https://linkedin.com/in/shubham)

*Built with â¤ï¸ to empower the Sign Language community.*
